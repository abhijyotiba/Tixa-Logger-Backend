# Central Logger

> **Provider-side log collection and analytics service for workflow monitoring**

A production-ready logging infrastructure that receives, stores, and serves workflow execution logs with built-in authentication, tenant isolation, and analytics capabilities.

---

## ğŸ—ï¸ Architecture

```
Client Workflow â†’ [HTTPS] â†’ Logger API â†’ PostgreSQL â†’ Dashboard
                    â†“
                 Auth + Validate + Store
```

**What this service does:**
- âœ… Accepts logs from workflow clients
- âœ… Authenticates via API keys
- âœ… Stores logs in PostgreSQL with JSONB
- âœ… Provides read APIs for dashboards
- âœ… Enforces tenant isolation

**What this service does NOT do:**
- âŒ Execute workflows
- âŒ Transform data
- âŒ Real-time streaming (yet)
- âŒ Heavy analytics (yet)

---

## ğŸš€ Quick Start

### 1. Prerequisites

- Python 3.10+
- PostgreSQL 14+ (or Supabase)
- pip

### 2. Installation

```bash
# Clone or navigate to project
cd central-logger

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Configuration

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your values
# Required: DATABASE_URL, API_KEYS
```

**Example .env:**
```env
DATABASE_URL=postgresql+psycopg://user:pass@localhost:5432/logger_db
SERVICE_NAME=central-logger
ENVIRONMENT=development
API_KEY_HEADER=X-API-Key
API_KEYS={"test_key_123": "client_1"}
```

### 4. Database Setup

```bash
# Initialize database tables
python -c "from app.db.database import init_db; init_db()"

# Or use Alembic for migrations (recommended for production)
alembic init migrations
alembic revision --autogenerate -m "Initial tables"
alembic upgrade head
```

### 5. Run the Server

```bash
# Development
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Production
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

Server will be available at: `http://localhost:8000`

API docs: `http://localhost:8000/docs` (development only)

---

## ğŸ“¡ API Endpoints

### Core Endpoints

| Method | Endpoint | Purpose | Auth |
|--------|----------|---------|------|
| `POST` | `/api/v1/logs` | Ingest single log | API Key |
| `POST` | `/api/v1/logs/batch` | Batch ingestion | API Key |
| `GET` | `/api/v1/logs` | Query logs (paginated) | API Key |
| `GET` | `/api/v1/logs/{id}` | Get log detail | API Key |
| `GET` | `/api/v1/metrics/overview` | Dashboard metrics | API Key |
| `GET` | `/api/v1/metrics/categories` | Category breakdown | API Key |
| `GET` | `/health` | Health check | None |

### Authentication

All API endpoints (except `/health`) require an API key:

```bash
curl -X POST http://localhost:8000/api/v1/logs \
  -H "X-API-Key: your_api_key_here" \
  -H "Content-Type: application/json" \
  -d '{...}'
```

---

## ğŸ“Š Database Schema

### `workflow_logs` table

```sql
id                       UUID PRIMARY KEY
client_id                TEXT (indexed)
environment              TEXT (indexed)
workflow_version         TEXT
ticket_id                TEXT (indexed)
executed_at              TIMESTAMPTZ (indexed)
execution_time_seconds   FLOAT
status                   TEXT (indexed)
category                 TEXT (indexed)
resolution_status        TEXT
metrics                  JSONB
payload                  JSONB
created_at               TIMESTAMPTZ (indexed)
```

**Indexes:**
- `idx_client_executed` (client_id, executed_at)
- `idx_client_status` (client_id, status)
- `idx_environment_executed` (environment, executed_at)

---

## ğŸ” Security

### API Key Management

**Development:**
- Store in `.env` file
- Format: `{"key": "client_id"}`

**Production:**
- Store in database with hashing
- Rotate regularly
- Use secrets manager (AWS Secrets Manager, etc.)

### Tenant Isolation

Every query automatically filters by `client_id`:
- Clients can only see their own logs
- Enforced at service layer
- No cross-tenant data leakage

### Best Practices

- âœ… Use HTTPS in production
- âœ… Rate limit ingestion endpoint
- âœ… Monitor for unusual patterns
- âœ… Log access attempts
- âœ… Regular security audits

---

## ğŸ§ª Testing

### Test Ingestion

```bash
# Test log ingestion
curl -X POST http://localhost:8000/api/v1/logs \
  -H "X-API-Key: test_key_123" \
  -H "Content-Type: application/json" \
  -d '{
    "environment": "production",
    "executed_at": "2025-12-24T10:30:00Z",
    "ticket_id": "TICKET-123",
    "status": "SUCCESS",
    "execution_time_seconds": 5.2,
    "category": "billing_issue",
    "metrics": {"confidence": 0.95},
    "payload": {"trace": []}
  }'
```

### Test Queries

```bash
# Get logs
curl -X GET "http://localhost:8000/api/v1/logs?page=1&page_size=10" \
  -H "X-API-Key: test_key_123"

# Get metrics
curl -X GET "http://localhost:8000/api/v1/metrics/overview?days=7" \
  -H "X-API-Key: test_key_123"
```

---

## ğŸ“¦ Deployment

### Option 1: Render

1. Create PostgreSQL database on Render
2. Create Web Service
3. Set environment variables
4. Deploy from GitHub

### Option 2: Railway

1. Create PostgreSQL addon
2. Create service from repo
3. Configure environment
4. Deploy

### Option 3: Docker

```dockerfile
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## ğŸ› Troubleshooting

### Database Connection Fails

- Check `DATABASE_URL` format
- Verify PostgreSQL is running
- Test connection: `psql $DATABASE_URL`

### API Returns 401

- Verify API key in `.env`
- Check `X-API-Key` header spelling
- Ensure key matches format: `{"key": "client_id"}`

### Logs Not Appearing

- Check client log shipper configuration
- Verify API endpoint URL
- Check server logs for errors
- Verify database connectivity

---

## ğŸ“ˆ What's Next

### Phase 4 Complete When:
- [x] API accepts logs
- [x] Authentication works
- [x] Logs stored in DB
- [x] Read endpoints functional

### Phase 5: Dashboard
- [ ] React/Next.js frontend
- [ ] Overview page
- [ ] Log list with filters
- [ ] Detailed trace viewer
- [ ] Category analytics

### Future Enhancements:
- Alerts and notifications
- Real-time streaming
- Advanced analytics
- ML-based insights
- Custom retention policies

---

## ğŸ“š Project Structure

```
central-logger/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ config.py            # Settings
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ ingest.py        # POST /logs
â”‚   â”‚   â”œâ”€â”€ read_logs.py     # GET /logs
â”‚   â”‚   â””â”€â”€ metrics.py       # GET /metrics
â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â””â”€â”€ api_key_auth.py  # Authentication
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ database.py      # DB connection
â”‚   â”‚   â””â”€â”€ models.py        # SQLAlchemy models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ log_service.py   # Business logic
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ validators.py    # Pydantic models
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## ğŸ¤ Support

For questions or issues:
1. Check logs: Look at server output
2. Review documentation
3. Test with curl commands
4. Verify environment configuration

---

## âš ï¸ Important Notes

- **Never commit `.env`** - Use `.env.example` as template
- **Tenant isolation is critical** - Always filter by client_id
- **Keep it simple** - Don't over-engineer early
- **Monitor performance** - Watch DB query times
- **Plan for scale** - But implement for today

---

**Built with:** FastAPI, PostgreSQL, SQLAlchemy, Pydantic

**Status:** âœ… Phase 4 Complete - Ready for Production Testing
